# STM32 AI Inference Example

This example shows how to run a tiny fully-connected neural network on an STM32 microcontroller without relying on the STM32Cube.AI code generator. The project is intentionally compact so that you can drop it into a CubeIDE workspace, adapt the HAL initialization to your board, and focus on integrating the model into your firmware.

## Highlights

- Minimal C implementation of a two-layer neural network with ReLU and softmax.
- Works on both the target MCU (build with the STM32 HAL) and a desktop host build for quick experimentation.
- UART logging that prints the predicted probabilities.
- Python utility that regenerates the model weights and biases from NumPy arrays.

## File structure

```
stm32_ai_example/
├── Core
│   ├── Inc
│   │   └── ai_model.h
│   └── Src
│       ├── ai_model.c
│       └── main.c
└── tools
    └── generate_example_weights.py
```

- `ai_model.c`/`ai_model.h` implement the neural network (4 inputs → 8 hidden units → 3 outputs).
- `main.c` contains a minimal STM32 entry point. The HAL initialization uses CubeMX-like helper functions that you can replace with code generated for your board.
- `tools/generate_example_weights.py` rebuilds the weight matrices if you want to experiment with different values.

## Building inside STM32CubeIDE

1. **Create a new project** in STM32CubeIDE that targets your board (for example NUCLEO-F401RE) and generate the HAL.
2. **Copy the example sources** into the generated project:
   - Add `Core/Inc/ai_model.h` to your project `Core/Inc` folder.
   - Add `Core/Src/ai_model.c` and `Core/Src/main.c`, replacing the autogenerated `main.c`.
3. **Enable the UART** you plan to use for logging (the example assumes USART2 at 115200 baud). Update `MX_USART2_UART_Init` if you use a different peripheral.
4. **Define the `STM32_TARGET` macro** in the project settings (`Project > Properties > C/C++ Build > Settings > Tool Settings > MCU GCC Compiler > Preprocessor`). This enables the HAL-specific code paths.
5. **Provide the correct clock configuration** by replacing the stub `SystemClock_Config` function with the version generated by CubeMX.
6. Build and flash the firmware. When the board resets, you should see serial output similar to:

   ```
   STM32 AI inference example
    AI output: [0.729, 0.040, 0.230]
   ```

## Desktop / host build (optional)

You can also compile and run the same inference logic on your development machine. This is useful to verify the math before flashing the MCU.

```bash
gcc -Istm32_ai_example/Core/Inc \
    stm32_ai_example/Core/Src/ai_model.c \
    stm32_ai_example/Core/Src/main.c -lm -o stm32_ai_demo
./stm32_ai_demo
```

Because the host build does not use the HAL, the program prints the inference results to `stdout` and then exits.

## Updating the model weights

`ai_model.c` ships with deterministic weights that map iris flower measurements to three class probabilities. To update the coefficients:

1. Edit `tools/generate_example_weights.py` to produce your desired NumPy arrays.
2. Run the script. It overwrites `Core/Src/ai_model.c` with new weight and bias values while preserving the inference logic.
3. Rebuild your firmware.

> **Tip:** If you train a model in Python (for example with scikit-learn or TensorFlow), export the dense layer weights as NumPy arrays and paste them into the script.

## UART integration tips

- Make sure that the GPIO pins corresponding to the UART peripheral are configured in alternate-function mode and that the clocks are enabled.
- If you use a different UART instance, change `huart2` and `USART2` to the peripheral available on your board.
- On boards without a dedicated USB-UART bridge you can redirect the output to SWO or semihosting by replacing the `uart_print` helper.

## License

This example inherits the repository license (see `LICENSE`).
